{"arxiv_id": "2511.19065v1", "num_chunks": 13, "chunks": ["Understanding, Accelerating, and Improving MeanFlow Training Jin-YoungKim* HyojunGo1\u2217 LeaBogensperger2 JuliusErbach1 NikolaiKalischek3 FedericoTombari3 KonradSchindler1 DominikNarnhofer1 1ETHZurich 2UniversityofZurich 3Google Abstract MeanFlow-XL 5 MeanFlow promises high-quality generative modeling in Ours-XL few steps, by jointly learning instantaneous and average velocity fields. Yet, the underlying training dynamics re- 4 main unclear. We analyze the interaction between the two 2.5x speed up velocities and find: (i) well-established instantaneous ve- 16% better locity is a prerequisite for learning average velocity; (ii) performance 3 learningofinstantaneousvelocitybenefitsfromaverageve- locitywhenthetemporalgapissmall, butdegradesasthe 50 75 100 125 150 175 200 225 250 gapincreases;and(iii)task-affinityanalysisindicatesthat Training Epochs smooth learning of large-gap average velocities, essential forone-stepgeneration,dependsonthepriorformationof accurate instantaneous and small-gap average velocities. Guidedbytheseobservations,wedesignaneffectivetrain- ingschemethatacceleratestheformationofinstantaneous velocity, then shifts emphasis from short- to long-interval averagevelocity. OurenhancedMeanFlowtrainingyields fasterconvergenceandsignificantlybetterfew-stepgenera- tion: WiththesameDiT-XLbackbone,ourmethodreaches an impressive FID of 2.87 on 1-NFE ImageNet 256\u00d7256, compared to 3.43 for the conventional MeanFlow base- Figure1. Ournovel, enhancedtrainingstrategyreachestheper- line. Alternatively, our method matches the performance formanceofMeanFlow-XLin\u22482.5\u00d7fewertrainingepochs,and converges to a final model with superior performance (\u224816% oftheMeanFlowbaselinewith2.5\u00d7shortertrainingtime, lowerFID). orwithasmallerDiT-Lbackbone. Ourcodeisavailableat https://github.com/seahl0119/ImprovedMeanFlow. singlestep. Earlyapproachesdistillfew-stepgenerativemodelsfrom 1.Introduction pretrained multi-step diffusion models, using direct [45, 56, 81], adversarial [59, 60, 76], or score-based supervi- Diffusion models [28, 61, 64] and Flow Matching [2, 39, sion[47,77,86]. Thetwo-stagedesignincreasescomplex- 40] have achieved state-of-the-art results across image [4, ity, requires two distinct training processes, and often de- 48, 75], video [1, 17, 70], and 3D generation [21, 22]. pends on large-scale synthetic data generation [40, 45], or However, it remains a persistent weakness that the de- onpropagationthroughteacher\u2013studentcascades[49,56]. noising relies on many, small iteration steps, making sam- Consistency models [66] represent a step towards one- pling computationally expensive [25]. Higher-order sam- time, end-to-end training, by enforcing consistent outputs plers [13, 32, 43, 44, 54, 62, 80] partially alleviate this, for all samples drawn along the same denoising trajectory. thoughachievinghighfidelitywithfewerthan10stepsre- Despitevariousimprovements[19,26,34,42,63,71,72],a mainsachallenge. Consequently,recentworkhasfocused substantialperformancegapremainsbetweenfew-stepcon- on models that enable inference in a few steps, or even a sistencymodelsandmulti-stepdiffusionmodels. Morere- *Equalcontribution centresearch[6,7,16,34,55,74,85]hasproposedtochar- 1 5202 voN 42 ]VC.sc[ 1v56091.1152:viXra )EFN-1( DIF LX-woLFnaeM LX-sruO acterizediffusion/flowquantitiesalongtwodistincttimein- largegaps,whicharethevitalingredientforfew-stepinfer- dices. Amongtheseattempts,MeanFlow[18]standsoutas ence. astableend-to-endtrainingschemethatmarkedlynarrows Empirically,theenhancedtrainingprotocolsubstantially thegapbetweenone-stepandmulti-stepgeneration. improvesthegenerationresultsandalsoacceleratesconver- ThekeytoMeanFlow\u2019ssuccessistheideaofexploiting gence. On the standard 1-NFE ImageNet [11] 256\u00d7256 theintrinsicrelationshipbetweentheinstantaneousvelocity benchmark, we improve the FID of MeanFlow-XL from (atasingletimepoint)andtheaveragevelocity(integrated FID 3.43 to 2.87, see Fig. 1. To reach the performance of overatimeinterval),suchthatasinglenetworklearnsboth conventionaltraining,ourimprovedtrainingschemeneeds simultaneously. However, MeanFlow training is computa- 2.5\u00d7 fewer iterations. Remarkably, it is even capable of tionallyexpensive,andhasonlybeenanalyzedrathersuper- matchingthatsameperformancewithasmallerDiT-Lback- ficially. Inparticular,itremainspoorlyunderstoodhowthe bone. twocoupledvelocityfieldsinteractduringlearningandhow Inabroadercontext,ourworkshowsthatthereisstilla their interplay can be coordinated to achieve high-quality lotofuntappedpotentialtoacceleraterecentfew-stepgen- one-stepgeneration. erativemodels.Withabetterunderstandingoftheirinternal Here, we investigate these learning dynamics and de- dynamics and numerical properties, high-quality real-time velop a training strategy that greatly improves both gen- generationmaywellbeachievable. eration quality and efficiency. Through controlled experi- ments, we determine that: (i) instantaneous velocity must 2.RelatedWork beestablishedearlyinthetrainingprocess,becauseitpro- Acceleration of diffusion and flow matching training. vides the foundation for learning average velocity: if in- Diffusion models [28, 61, 64] gradually perturb data with stantaneousvelocitiesarepoorlyformedorcorrupted,then noise and train a network to reconstruct the clean signal. learningaveragevelocitiesfailsaltogether;(ii)intheoppo- Thisnoise-injectionanddenoisingprocesscanbedescribed site direction, the time interval over which average veloci- via stochastic differential equations (SDEs) or, equiva- tiesarecomputed(the\u201ctemporalgap\u201d)criticallydetermines lently, as a probability-flow ordinary differential equation how they impact the learning of instantaneous velocities: (ODE) [32, 65]. Flow Matching", "stantaneousvelocitiesarepoorlyformedorcorrupted,then noise and train a network to reconstruct the clean signal. learningaveragevelocitiesfailsaltogether;(ii)intheoppo- Thisnoise-injectionanddenoisingprocesscanbedescribed site direction, the time interval over which average veloci- via stochastic differential equations (SDEs) or, equiva- tiesarecomputed(the\u201ctemporalgap\u201d)criticallydetermines lently, as a probability-flow ordinary differential equation how they impact the learning of instantaneous velocities: (ODE) [32, 65]. Flow Matching [2, 39, 40] extends this small gaps facilitate instantaneous velocity formation and with velocity fields, enabling the model to learn transport refinement,whilelargegapsdestabilizeit;(iii)taskaffinity pathsconnectingdataandreferencedistributions. analysisrevealsthatoneshouldinitiallyfocusonsmall-gap Training these models is computationally expen- averagevelocities,whichlaythefoundationforlearningthe sive [52]. To accelerate it, prior work mostly focuses large-gap average velocities that are required for one-step on critical timesteps using two strategies: (1) timestep- generation. dependent loss weighting based on SNR [25], perceptual Standard MeanFlow training ignores these subtle, but quality[10],oruncertainty[20,33],and(2)modifiedsam- impactfuldynamics. Throughoutthetrainingprocess,itap- pling distributions [82\u201384]. Furthermore, there are hybrid plies the same, fixed loss function and sampling scheme, approaches that combine the two [73], as well as adaptive disregardingthecomplexdependenciesbetweenthetwove- schedulers[35]. locity fields. This naive training objective interferes with Wewillleveragesomeoftheexistingaccelerationtech- the early formation of reasonable instantaneous velocities, niquestospeeduptheformationofinstantaneousvelocity, whichinturndelaysthelearningofaveragevelocities. Ul- whichisimportanttospeedupconvergence,seeSec.4. timately,thecurrenttrainingpracticesignificantlydegrades overallperformancecomparedtowhatwouldbeachievable Few-step generative models. Early work on few-step with a given model and dataset, and also slows down the models revolves around distillation [45], progressive [5, training. 56],data-freeapproaches[23,49,87],andvariousformsof To remedy these issues, we propose a simple yet effec- alternative supervision,e.g. adversarial [59, 60, 76], score- tive extension of MeanFlow training. To quickly estab- based [47, 77, 86], moment matching [57], operator learn- lishreasonableinstantaneousvelocities,weadoptaccelera- ing [81], and physics-informed losses [68]. These are not tiontechniquesfromdiffusiontraining[10,20,25,35,73]. end-to-end pipelines; they are two-stage (or tightly sched- To support the learning of correct average velocities, we uled) procedures that require manually selected transition design a progressive weighting scheme. In early training points\u2014i.e.,decidingwhentostopteachertrainingandbe- stages, the weighting prioritizes small gaps, which rein- gindistillation[16]. forces instantaneous velocity formation and prepares the Consistency models [66] pioneered a major advance, ground for large-gap learning. As training progresses, the enabling end-to-end training of few-step generators, and weighting gradually transitions to equal weighting across theyhavealsobeenusedindistillationsetups[38,41,78]. all gap lengths, ensuring accurate average velocities over The core principle is to enforce that model outputs from 2 any two points along the same trajectory are identical. locity(v)attimetandtolearnanaveragevelocity(u)be- This foundational concept has inspired a wide range of tweentimepointsrandtdefinedbytheMeanFlowidentity follow-ups, such as improving training stability and sim- 1 (cid:90) t plicity [19, 42, 63, 71], extending the framework to multi- u(z t,r,t)\u225c t\u2212r v t(z \u03c4,\u03c4) d\u03c4. (2) step[26,34,72],adaptingtolatentspacemodels[46],and r Bylearninguwithaneuralnetworku ,MeanFlowapprox- \u03b8 incorporating adversarial loss [34, 36]. Despite these ad- imatesthefinite-timeODEintegralinEq.1,enablingasin- vances,asubstantialgapremainsbetweenthefew-stepper- gleupdatestepz = z \u2212(t\u2212r)u (z ,r,t)thatreplaces r t \u03b8 t formance of end-to-end training and the performance of multiplesmallsolversteps. Totrainu ,MeanFlowexploits \u03b8 fullymulti-stepdiffusionmodels. therelationshipbetweenuandv,whichisdefinedas: Morerecently,severalworkshaveproposedtolearndif- u(z ,t,r)=v (z ,t)\u2212(t\u2212r)(v (z ,t)\u2202 u +\u2202 u ). (3) fusion and flow quantities between two time points [6, 7, t t t t t x \u03b8 t \u03b8 16, 34, 55, 74, 85]. For example, Flow Maps [6] define Theoverallobjectiveisthengivenby: the integral of the flow between time points and learn it L =E [\u2225u (z ,r,t)\u2212sg(u )\u22252], (4) via matching losses. Shortcut Models [16] augment flow MF x,\u03f5,t,r \u03b8 t tgt 2 whereu =v (z ,t)\u2212(t\u2212r)(v", "t \u03b8 16, 34, 55, 74, 85]. For example, Flow Maps [6] define Theoverallobjectiveisthengivenby: the integral of the flow between time points and learn it L =E [\u2225u (z ,r,t)\u2212sg(u )\u22252], (4) via matching losses. Shortcut Models [16] augment flow MF x,\u03f5,t,r \u03b8 t tgt 2 whereu =v (z ,t)\u2212(t\u2212r)(v (z ,t)\u2202 u +\u2202 u )and matchingwitharegularizationlosstolearnshortcutpaths, tgt t t t t z \u03b8 t \u03b8 sg(\u00b7)denotesthestop-gradientoperation. andInductiveMomentMatching(IMM)[85]enforcesself- The training strategy proposed by [18] samples a por- consistencyofstochasticinterpolantsacrosstime. tionoftheminibatchwitht = r, inwhichcasetheMean- Among the methods developed so far, MeanFlow [18] FlowobjectiveinEq.4reducestoflowmatchingbylearn- standsout:itsubstantiallynarrowstheperformancegapbe- ing the instantaneous velocity v. Hence, their loss can be tweenfew-stepandfullmulti-stepdiffusionmodels, while interpretedasthesumoftwotermsfortheaveragevelocity beingtrainedend-to-end. Theaimofthepresentpaperisto L (z ,r,t)andtheinstantaneousvelocityL (z ,t): understand, improve, and accelerate the training of Mean- u t v t Flow. There are a few concurrent efforts: AlphaFlow [79] L =E [L (z ,r,t)\u00b7I(t\u0338=r)+L (z ,t)\u00b7I(t=r)]. MF x,\u03f5,t,r u t v t replaces the original MeanFlow objective with a softened (5) one. In contrast, we retain the MeanFlow formulation and 4.Observations improveitbasedonacarefulempiricalanalysis. CMT[30] splits learning into multiple stages, whereas our method AsdescribedinSection3,theMeanFlowobjectivedecom- preservesthesimplicityofend-to-endMeanFlowtraining. poses into learning instantaneous velocity v and average velocity u. In the following, we study how these cou- 3.Background: MeanFlow pled quantities interact during training and how to opti- Flowmatching. Flowmatching[2,39,40]learnsatime- mize their interplay to maximize one-step generation per- dependent vector field v (z ,t) that transports a (typically formance. For these experiments, we use a DiT-B/4 [51] \u03b8 t Gaussian)sourcedistribution\u03f5\u223cp (\u03f5)toatargetdatadis- architectureandtheImageNet256\u00d7256dataset[11]. Fur- 1 tribution x \u223c p (x). This transformation is defined as the therdetailsareprovidedinAppendixA. 0 solutiontotheODEthatcharacterizestheflow\u03a6 4.1.Impactofv-Learningonu-Learning d\u03a6 (z)=v (\u03a6 (z)). (1) We first investigate how learning the instantaneous veloc- dt t t t ity v affects the quality of the learned average velocity u. Avalidvelocityfieldcanbelearnedbyoptimizingthecon- Through controlled experiments, we demonstrate that es- ditionalflowmatchingobjective, whichutilizesatractable tablishingvisaprerequisiteforu-learning. conditional velocity v (z |\u03f5) instead of the intractable t t Learning v facilitates learning u. We examine whether marginalvelocityv (z )[39].Differentchoicesforthecon- t t andhowinstantaneousvelocityvaffectsthelearningofthe ditionalflowpathsarepossible, arguably, thesimplestand average velocity u. We conduct a two-stage training: we mostpopularonesareoptimaltransportconditionalveloc- first train the model with v-loss (L (z ,t) in Eq. 5) and v t ityfields. Givenapair(x,\u03f5)andatimet \u2208 [0,1],z isthe t thenfinetuneitwithu-loss(L (z ,r,t)inEq.5). Thisex- u t linearinterpolantz =(1\u2212t)x+t\u03f5,whoseconditionalve- t perimental setup makes it possible to analyze what effect locity[39]isthetimederivativev (z |\u03f5)=z\u02d9 =\u03f5\u2212x. The t t t v-pretraining has on learning u. We use 1-NFE FID as an neural network v is by minimizing the conditional flow- \u03b8 evaluationmetricforthequalityofu. matching loss L = E [\u2225v (z ,t)\u2212v (z |\u03f5)\u22252]. CFM x,\u03f5,t \u03b8 t t t 2 Figure 2 shows the results of two complementary set- Datasamplesaregeneratedbysolvingtheprobability-flow tings. In the first setting (top), we fix the budget for fine- ODEinEq.1. tuning u at 60 epochs while varying the duration of v- MeanFlow. The core idea of MeanFlow [18] is to inter- pretraining between {0, 5, 10, 15, 20} epochs. In the sec- pret the flow-matching velocity as the instantaneous ve- ond setting (bottom), we fix the total training budget at 3 v-0 v-5 v-10 v-15 v-20 -0 -40 v v 30 200 Better 25 100 convergence 50 -40: 34.36", "to inter- pretraining between {0, 5, 10, 15, 20} epochs. In the sec- pret the flow-matching velocity as the instantaneous ve- ond setting (bottom), we fix the total training budget at 3 v-0 v-5 v-10 v-15 v-20 -0 -40 v v 30 200 Better 25 100 convergence 50 -40: 34.36 20 v 20 10 20 30 40 50 60 Training Epochs 10 0.1-0.3 0.3-0.5 0.5-0.7 0.7-0.9 60 21 =t r 20 Faster convergence Figure 4. Impact of \u2206t of u-learning on v-learning. 32-NFE 19 FID after 40 epochs of u finetuning across different \u2206t ranges, 40 18 69 71 73 75 77 79 starting from either random initialization (blue) or v-pretrained 20 model (orange, 40 epochs). Small \u2206t enables constructing and 10 20 30 40 50 60 70 80 improvingv,whilelarge\u2206tdegradespretrainedv.Thegreenline Training Epochs denotestheperformanceofthev-pretrainedmodel. Figure 2. v-learning facilitates u-learning. (Top) 1-NFE FID duringu-finetuningaccordingtov-pretrainingepochs. (Bottom) 1-NFEFIDunderafixed80-epochbudgetwithvaryingallocation 0.6 betweenv-pretrainingandu-finetuning. Bothsettingsshowthat 0.5 investinginv-learningimprovesu-learningquality. v-0 0.4 v-40 0.3 u-40 t [0.1,0.3] 80 70 0.1-0.3 0.3-0.5 0.5-0.7 0.7-0.9 t=t r 60 Figure 5. Task affinity between v- and u-learning across \u2206t 50 k=0.03 ranges. Small-\u2206tu-pretrainingachieveshigheraffinityforlarge 40 k=0.05 30 k=0.1 \u2206tcomparedtov-pretraining,providingabetterregimeforlearn- 20 k=0.2 k=0:18.87 inglarge-gapaveragevelocitywithinstantaneousvelocity. 10 20 30 40 50 60 70 80 Training Epochs words, a corrupted instantaneous velocity makes learning Figure 3. Corruption in v-learning disrupts u-learning. 1- oftheaveragevelocityalotharder. NFEFIDwhentrainingwithL whileinjectingGaussiannoise MF Implication. Thetwoexperimentsaboverevealsymmet- scaledbyk\u00b7\u2225v (z |\u03f5)\u2225intothetargetvelocityofL . Evensmall t t v ric dependencies: u-learning benefits from well-formed v, noise (k = 0.03) disrupts v-learning and severely degrades u- while failing when v is corrupted. This aligns with the learningperformancecomparedtocleantraining(k=0). mathematical structure of MeanFlow\u2014u is defined as the temporalintegralofv,sothelattermustbereasonablywell 80 epochs and allocate {0, 5, 10, 15, 20} epochs to v- establishedtolearntheformer. Thesefindingssuggestthat pretraining, with the remainder dedicated to u-finetuning. instantaneous velocity should be prioritized early in train- Clearly, investing more into v-pretraining yields more sta- ing,tolaythefoundationforsubsequentlearningoftheav- bleandaccurateu-learning. Inthesecondsetting,evenun- eragevelocity. der a fixed compute budget, prioritizing v earlier is more effective and accelerates convergence. Overall, the results 4.2.Impactofu-Learningonv-Learning suggestthatawell-formedv isanecessaryprerequisitefor Complementary to Sec. 4.1, we analyze how the temporal subsequentu-learning. gap\u2206t = t\u2212r inaveragevelocitysupervisioninfluences Corruption in v-learning disrupts u-learning. To ex- theinstantaneousvelocity. Weconsidertwoinitializations: amine the opposite case, we investigate whether u can (1) a model pretrained with v-loss for 40 epochs, and (2) be accurately learned when v-learning is deliberately cor- randominitialization. Bothmodelsarethenfinetunedwith rupted. DuringMeanFlowtraining, weintentionallyinject theu-lossfor40epochswhilerestricting\u2206ttooneoffour Gaussiannoise(scaledbyk\u00b7\u2225v (z |\u03f5)\u2225)intothetargetcon- ranges: [0.1,0.3],[0.3,0.5],[0.5,0.7],or[0.7,0.9]. Theex- t t ditionalvelocityofthev-loss,therebydegradingv-learning perimentaldesignexposeshowu-supervisionwithdifferent while leaving the u-loss intact. We again use 1-NFE FID \u2206t modifies a pretrained instantaneous velocity, and how asanevaluationmetricforu-learningqualityandillustrate effectivelyitformsonefromscratch. Tomeasurethequal- theresultsacrossnoisescalesk inFig.3. Evenwithsmall ityofv,wesetv(z ,t) = u (z ,t,t)andevaluate32-NFE t \u03b8 t noise (k = 0.03), u-learning severely degrades. In other FID,seeresultsinFig.4. 4 DIF DIF DIF )derp- ,1=EFN( )derp- ,1=EFN( )derp- ,1=EFN( DIF SAT )derp-v ,23=EFN( Small\u2206tsupervisionformsv. Restrictingu-learningto Amongthetwo,pretrainingwithsmall-\u2206tsupervisionex- small temporal gaps (\u2206t \u2208 [0.1,0.3]) reveals two notable hibitsastrongeraffinityforlarge-\u2206tregimescomparedto effects(Fig.4).First,amodeltrainedfromscratchwiththis pure v-loss pretraining, indicating that small-\u2206t supervi- u-lossachievesa32-NFEFID,comparableto40epochsof sion provides a more favorable initialization for the later v-pretraining(greenline),demonstratingthatsmall-\u2206tsu- learningstagesthatextendutolargetemporalgaps. pervision is a viable proxy for v-learning. Second, when Implication. Small-\u2206t supervision (Strategy 2) creates finetuning the v-pretrained model, the same u-loss yields a better initialization for the challenging large-\u2206t", "effects(Fig.4).First,amodeltrainedfromscratchwiththis pure v-loss pretraining, indicating that small-\u2206t supervi- u-lossachievesa32-NFEFID,comparableto40epochsof sion provides a more favorable initialization for the later v-pretraining(greenline),demonstratingthatsmall-\u2206tsu- learningstagesthatextendutolargetemporalgaps. pervision is a viable proxy for v-learning. Second, when Implication. Small-\u2206t supervision (Strategy 2) creates finetuning the v-pretrained model, the same u-loss yields a better initialization for the challenging large-\u2206t regime additional FID gains, indicating that it also improves the thanpurev-loss(Strategy1). Hence,small-\u2206tsupervision pretrained instantaneous velocity. Together, these results shouldbeincludedearlyintraining,preparingforthesub- demonstratethatsmall-\u2206tsupervisionprovidesaneffective sequent stages that learns u over large temporal gaps\u2014the learningsignalfortheinstantaneousvelocityv. modeultimatelyrequiredforone-stepgeneration. Large \u2206t supervision deteriorates v. In contrast, u- 5.Method learning with larger temporal gaps (\u2206t \u2208 [0.3,0.5], [0.5,0.7], [0.7,0.9]) yields poor 32-NFE FID, with both In Sec. 4, we made three key observations: (O1) instanta- initializations. i.e., large-\u2206t supervision is of limited use neousvelocity(v)mustbeestablishedearly, asitprovides to construct v from scratch and, even worse, seriously de- thefoundationforlearningaveragevelocity(u)\u2014whenvis gradesanalreadypretrainedinstantaneousvelocity. poorlyformedorcorrupted,u-learningfails;(O2)thetem- Implication. When training is not properly managed, poralgapdetermineshowu-learningaffectsv: smalltem- there is a self-destructive dynamic: u-learning requires a poral gaps facilitate formation and refinement of v, while stable v foundation, yet large-\u2206t supervision destabilizes largetemporalgapsdegradeit;and(O3)taskaffinityanal- it. Together with the earlier finding that v should be es- ysis reveals that including small-gap supervision creates a tablishedearly(Sec.4.1), thistranslatesintothefollowing more favorable initialization for learning the large-gap av- guideline: suppress large-\u2206t supervision in early training eragevelocityrequiredforone-stepgeneration. stages,toavoiddisruptingv-learning. TheoriginalMeanFlowobjectivedoesnotconsiderthese properties and thus trains with standard v-loss and the 4.3.TaskAffinityAnalysis wholerangeof\u2206tfromthestart. Thus,thetrainingsuffers fromtheobservedinefficiencies: itfailstorapidlyformv, From Sec. 4.1, we observe that v should be established whichinturndelayslearningofu,sinceitdependsonwell- earlyasafoundationforu-learning. However,asshownin formed instantaneous velocities; resulting in slow training Sec.4.2, supervisionwithlargetemporalgapsdestabilizes andsuboptimalmodelperformance. v,whilesupervisionwithsmalltemporalgapscanactually We translate our insights into a unified training strat- benefit v learning. Therefore, we should exclude large-\u2206t egy that directly addresses these limitations, thereby ac- supervisionfromtheearlytrainingphase,whichshouldbe celerating convergence and improving overall generation dedicatedtoformingv. Inthisregard,twoviablestrategies quality. Our strategy has two components: (1) the use of emerge: (1)purev-learningviav-loss,or(2)combiningu- well-established training acceleration techniques for diffu- loss with small temporal gap supervision, which serves as sionandflowmodelstorapidlyformv,and(2)progressive aneffectivesignalforv-learning. weightingoftheu-loss(L (z ,r,t)),suchthatitprioritizes u t To determine whether pure v-loss or small-\u2206t u-loss small-\u2206tearlyintrainingandgraduallytransitionstouni- better prepares the ground for large-\u2206t learning, we re- formweightingacrossall\u2206t. sort to the Task Affinity Score (TAS) [15, 20, 67]. TAS Acceleratingv-learning. Torapidlyestablishv,weadopt measures how smoothly two tasks can be jointly trained, twostandard accelerationtechniques: specialized timestep quantifying their lack of conflict through training itera- tions. We compute this score between v-loss and u-loss samplingandtime-dependentlossweighting. Fortimestep (across different \u2206t-ranges) under three distinct initializa- sampling, we replace the base sampling in Eq. 4 with a tion schemes: baseline: random initialization; strategy 1: custom distribution p acc(t). For loss weighting, we ap- pretrainedwithv-lossfor40epochs;strategy2: pretrained ply a time-dependent weight \u03b1(t) to the v-loss, modifying with u-loss (\u2206t \u2208 [0.1,0.3]) for 40 epochs. The results, the corresponding term in Eq. 4 to \u03b1(t) \u00b7 L v(z t,t). The showninFig.5,clarifyhowtobestinitializelarge-\u2206tlearn- specific forms of p acc(t) and \u03b1(t) are adopted from estab- lishedaccelerationmethods[10,20,25,33,73,82\u201384]and ing. are designed to focus the model training on more critical Themodelspretrainedwithstrategies1and2bothhave timesteps,therebyacceleratingconvergence. higher TAS than random initialization across all temporal gap ranges, showing that both pretraining strategies cre- ProgressiveL weighting. WeweightL (z ,r,t)toem- u u t ate a more favorable regime for joint MeanFlow learning. phasize small temporal", "lishedaccelerationmethods[10,20,25,33,73,82\u201384]and ing. are designed to focus the model training on more critical Themodelspretrainedwithstrategies1and2bothhave timesteps,therebyacceleratingconvergence. higher TAS than random initialization across all temporal gap ranges, showing that both pretraining strategies cre- ProgressiveL weighting. WeweightL (z ,r,t)toem- u u t ate a more favorable regime for joint MeanFlow learning. phasize small temporal gaps early in training, then gradu- 5 1-NFE 2-NFE Method #Params NFE FID\u2193 Method #Params Epoch FID\u2193 FID\u2193 GANs/NormalizingFlows ComparisontoMeanFlowacrossmodelsizes BigGAN[8] 112M 1 6.95 MeanFlow-B/4 131M 240 11.58 7.85 StyleGAN-XL[58] 166M 1 2.30 +OurswMinSNR 131M 240 9.87 7.08 GigaGAN[31] 569M 1 3.45 +OurswDTD 131M 240 10.20 7.31 STARFlow[24] 1.4B 1 2.40 MeanFlow-M/2 308M 240 5.01 4.61 Autoregressive/Maskingmodels +OurswMinSNR 308M 240 4.61 4.30 VQ-GAN[14] 227M 1024 26.52 +OurswDTD 308M 240 4.43 4.10 MaskGIT[9] 227M 8 6.18 MeanFlow-L/2 459M 240 3.84 3.35 VAR[69] 2B 10\u00d72 1.92 +OurswMinSNR 459M 240 3.79 3.31 MAR-H[37] 943M 256\u00d72 1.55 +OurswDTD 459M 240 3.47 3.24 Diffusion/Flowmodels Few-stepdiffusion/flowmodelsfromend-to-endtraining ADM[12] 554M 250\u00d72 10.94 iCT-XL/2\u2021[63] 675M - 34.24 20.30 LDM[53] 400M 250\u00d72 3.60 Shortcut-XL/2[16] 675M 160 10.60 - U-ViT-H/2[3] 501M 50\u00d72 2.29 iMM-XL/2\u2020[85] 675M 3840 - 7.77 SimDiff[29] 2B 512\u00d72 2.77 MeanFlow-XL/2+[18] 676M 1000 - 2.20 DTR-L/2[50] 458M 250\u00d72 2.33 MeanFlow-XL/2 676M 240 3.43 2.93 DiT-XL/2[51] 675M 250\u00d72 2.27 +OurswDTD 676M 240 2.87 2.64 SiT-XL/2[48] 675M 250\u00d72 2.06 Table1.Resultsforclass-conditionalgenerationonImageNet256\u00d7256.(Left)Comparisonoffew-stepdiffusion/flowmodels.(Right) Othergenerativemodelfamiliesasreference.\u201c\u2020\u201dinlefttableand\u201c\u00d72\u201dinrighttableindicatethatClassifier-FreeGuidance(CFG)doubles NFEpersamplingstep.\u2021:iCTresultsfrom[85]. allytransitiontouniformweighting. Specifically,weuse erationat256\u00d7256resolutionwithDiTarchitectures[51]. Tomeasurefew-stepgenerationperformance,weutilizethe \u03b2(\u2206t,s)=1\u2212s+\u03bbs(1\u2212\u2206t), FID [27] score on 50K samples from either 1-NFE or 2- NFEgeneration. where s \u2208 [0,1] denotes the training progress. At initial- izations = 1and\u03b2(\u2206t,1) = \u03bb(1\u2212\u2206t)prioritizessmall Implementationdetails. Wetestseveralwaysofacceler- \u2206t;atconvergence\u03b2(\u2206t,0) = 1weightsallgapsequally. atingvelocitytraining,onemethodfromeachcategory. For To maintain a uniform expectation at initialization, we set theirsimplicityandgoodempiricalperformance,wechoose \u03bb = 1/E [1 \u2212 \u2206t]. We use a simple linear schedule \u2206t MinSNR[25]aslossweightingapproach,andDTD[35]as s = 1\u2212i/T, where i and T denote the current iteration timestep sampling approach. More sophisticated methods and the total number of iterations, respectively. A slower exist[20,73,84],butwedonotexpectlargedifferencesin initialtransitioncanbeachievedbysettings=1\u2212(i/T)k thecontextofourprogressiveweightingscheme. withk >1;conversely,k <1yieldsafastertransition. Integration with MeanFlow components. MeanFlow 6.2.ComparativeEvaluation training employs a number of stabilization techniques, in- Comparison to MeanFlow. As shown in Table 1 (top cludingspecializedlossmetricsandsamplingstrategies. In left),wefirstcompareourmethodagainstMeanFlowusing AppendixBweprovidedetailedinstructionsonhowtoin- the DiT-B/4, DiT-M/2, and DiT-L/2 models. Our method tegrateourproposedadaptationswiththesecomponents. consistently outperforms MeanFlow, regardless of the ve- locity learning acceleration technique employed. When 6.Experiments comparing the two acceleration methods, MinSNR and Inthefollowing,weshowthatourmethodacceleratescon- DTD, a clear pattern emerges: MinSNR surpasses DTD vergence and attains higher final performance. Together, on DiT-B/4, but this advantage diminishes at larger scales theseresultsshowthatourobservationstranslateintoprac- (L/2, M/2). We attribute this discrepancy to MeanFlow\u2019s ticaltrainingimprovements. adaptive loss weighting, which normalizes loss values by theirnormtobalancetheirinfluence.Loss-weightingstrate- 6.1.ExperimentalSetup gies like MinSNR interfere with this adaptive mechanism, We follow the original experimental setup of Mean- consequently reducing robustness across different model Flow[18]andconductexperimentsonImageNet[11]gen- scales. In contrast, timestep-sampling methods like DTD 6 MeanFlow MeanFlow + Ours w MinSNR MeanFlow + Ours w DTD 6 7 17 5 6 15 2.1x speed up 13 4 5 1.5x speed up 2.3x speed up 11 3 4 9 75 100 125 150 175 200 225 250 75 100 125 150 175 200 225 250 100 120 140 160 180 200 220 240 Training Epochs Training Epochs Training Epochs (a)DiT-L/2 (b)DiT-M/2 (c)DiT-B/4 Figure6.ConvergencespeedcomparisonbetweenMeanFlowandourmethodsacrossmodelsizes. 30 epochs 60 epochs 120 epochs", "1.5x speed up 2.3x speed up 11 3 4 9 75 100 125 150 175 200 225 250 75 100 125 150 175 200 225 250 100 120 140 160 180 200 220 240 Training Epochs Training Epochs Training Epochs (a)DiT-L/2 (b)DiT-M/2 (c)DiT-B/4 Figure6.ConvergencespeedcomparisonbetweenMeanFlowandourmethodsacrossmodelsizes. 30 epochs 60 epochs 120 epochs 240 epochs 30 epochs 60 epochs 120 epochs 240 epochs Figure7.QualitativecomparisonofgeneratedsamplesacrosstrainingepochsonDiT-XL/2. only modify the sampling distribution and leave the loss CFGSetup Method FID(1-NFE)\u2193 FID(2-NFE)\u2193 weightingschemeintact,thuspreservingcompatibilitywith MeanFlow-L/2 4.60 4.58 MeanFlow\u2019s adaptive design. Therefore, due to its consis- \u03ba=0.5 +OurswDTD 4.08 3.97 tentperformanceacrossmodelsizes,weselectDTDasour \u03c9=2.0 MeanFlow-XL/2 4.72 4.64 primaryaccelerationmethod. +OurswDTD 4.31 4.29 \u03ba=0.92 MeanFlow-L/2 3.84 3.35 ImageNet 256\u00d7256 benchmark. We scale up our \u03c9=2.5 +OurswDTD 3.47 3.24 method with DTD to the DiT-XL model and compare it \u03ba=0.92 MeanFlow-XL/2 3.43 2.93 with previous one- and few-step diffusion/flow models in \u03c9=2.5 +OurswDTD 2.87 2.64 Table1(bottomleft). OurmethodalsooutperformsMean- Table 2. Robustness to CFG configurations. Performance of Flow in the DiT-XL setup (240 epochs), improving Mean- DiT-LandDiT-XLwithdifferentCFGsetups. Ourmethodcon- Flow\u2019s 1-NFE FID from 3.43 to 2.87 and its 2-NFE FID sistentlyimprovesacrossallconfigurations. from 2.93 to 2.64. We highlight that this improvement on DiT-XL(3.43\u21922.87),anotable16%reductioninFID,sub- stantiallynarrowsthegapbetweenone-stepgenerationand generatesnoticeablysharperandmoredetailedimagesthan multi-stepdiffusionmodels,asshowninTable1(right). vanillaMeanFlow. Remarkably,oursamplesat120epochs exhibit quality comparable to or exceeding vanilla Mean- Convergence speed. Next, we compare the convergence Flow\u2019s 240-epoch results, demonstrating \u22482\u00d7 faster con- behaviorofourmethodagainstvanillaMeanFlowtraining, vergence. Together, these quantitative and qualitative re- both quantitatively and qualitatively. As shown in Fig. 1 sultsconfirmthatourapproachnotonlysubstantiallyaccel- and 6, our approach achieves substantially faster conver- eratestrainingbutalsoimprovesfinalgenerationquality. gence across all model sizes. Notably, our method with DTD demonstrates superior convergence on DiT-XL/2, - 6.3.AblationandAnalysis L/2, and -M/2, achieving approximately 2.5\u00d7, 2.3\u00d7, and 2.1\u00d7speedup,respectively. Effectiveness across different CFG configurations. Qualitatively, Fig. 7 shows samples from DiT-XL/2 at MeanFlow integrates Classifier-Free Guidance (CFG) by differenttrainingepochs. Atequivalentepochs,ourmethod mixingthreecomponentsinthetargetu : conditionalve- tgt 7 )1=EFN( DIF LX-wolFnaeM LX-wolFnaeM DTD-sruO + )1=EFN( DIF 1=EFN( DIF Method FID(1-NFE)\u2193 FID(2-NFE)\u2193 MeanFlow-B/4+OurswDTD MeanFlow-B/4 MeanFlow-B/4 11.58 7.85 k=0.5 k=1 k=2 k=3 FID\u2193(1-NFE) 11.58 11.16 10.20 11.44 11.99 +MinSNR 10.57 7.38 +DTD 10.96 7.55 Table 4. Effect of k. 1-NFE FID with varying settings of the scheduleparameterk. Thelinearschedule(k = 1)achievesthe +L weighting. 10.98 7.58 u lowestFID. +MinSNR+L weighting. 9.87 7.08 u +DTD+L weighting. 10.20 7.31 FID\u2193 u Method Table3. Ablationofmethodcomponents. Velocityacceleration 32-NFE 64-NFE 128-NFE methodsandL weightingeachimproveuponvanillaMeanFlow u MeanFlow 7.61 7.26 7.16 training,withtheircombinationachievingthebestperformance. +OurswMinSNR 7.09 6.91 6.86 +OurswDTD 7.25 7.01 6.93 locity, conditional velocity prediction, and unconditional Table5. Comparisonofmulti-stepgenerationwithestimated velocity prediction, using coefficients \u03c9 and \u03ba (see Ap- instantaneousvelocityonDiT-B/4models. pendixAin[18]). Forsmallermodels(DiT-BandDiT-M), \u03ba = 0.5 and \u03c9 = 2.0 are used, while larger models (DiT- comparedifferentk valuesonDiT-B/4inTable4. There- L and DiT-XL) use \u03ba = 0.92 and \u03c9 = 2.5. Additionally, sultsshowthatk =1achievesthebestperformance(10.20 for DiT-L and DiT-XL, CFG mixing is only applied when FID),validatingourchoiceofasimplelinearschedule. We t is sampled within specific ranges ([0.0, 0.8] for DiT-L, observethatslowertransitions(k <1)andfastertransitions [0.0,0.75]forDiT-XL).Toverifyourmethod\u2019srobustness (k > 1)bothdegradeperformance, indicatingthatalinear to these configurations, we train DiT-L and DiT-XL using scheduleprovidesagoodbalance. theCFGsettingsofboththesmallandlargemodels. Enhanced instantaneous velocity. We argue that accel- Table 2 shows that our method consistently improves erating instantaneous velocity formation is crucial for ef- performance across all CFG configurations. This demon- fective training (Sec. 4). To verify that the acceleration strates the robustness", "we train DiT-L and DiT-XL using scheduleprovidesagoodbalance. theCFGsettingsofboththesmallandlargemodels. Enhanced instantaneous velocity. We argue that accel- Table 2 shows that our method consistently improves erating instantaneous velocity formation is crucial for ef- performance across all CFG configurations. This demon- fective training (Sec. 4). To verify that the acceleration strates the robustness of our method to different guidance methods improve v quality, we evaluate multi-step gener- scenarios,underscoringitsroleasasimplebuthighlyeffec- ationperformanceusingu (z ,t,t)asthevelocityestimate \u03b8 t tivetechniqueforachievingconsistentperformancegains. (whichequalsthemodel\u2019sinstantaneousvelocityprediction Effectofindividualcomponents. Tovalidatethecontri- when r = t). As shown in Table 5, our methods consis- butionofeachcomponent,wetrainDiT-B/4withthreecon- tentlyoutperformMeanFlowacrossallsamplingsteps(32, figurations: (1) applying only velocity acceleration meth- 64, 128 NFE) regardless of the applied velocity accelera- ods (MinSNR or DTD), (2) applying only progressive L tion methods. This demonstrates that our training strategy u weighting,and(3)thefullmethodcombiningbothcompo- actuallyimprovestheunderlyinginstantaneousvelocityes- nents. Allmodelsaretrainedfor240epochs,matchingthe timated by the model, confirming that our approach yields setupinSec.6.2. TheresultsareillustratedinTable3. higher-qualityvelocityfields. Asshownintheresults,onlyapplyingvelocityaccelera- tionmethodsimprovesvanillaMeanFlowtraining(11.58\u2192 7.Conclusion 10.57 with MinSNR, 10.96 with DTD at 1-NFE), demon- In this work, we revisited MeanFlow through a detailed strating that rapid v formulation benefits training. More- analysis of its instantaneous and average velocity compo- over,applyingonlyprogressiveweightingonL improves u nents and their interaction during training. We identified theperformanceto10.98at1-NFE,showingthebenefitof key learning dynamics: instantaneous velocity should be proper temporal gap scheduling. The combined approach establishedearlytoenableeffectiveaveragevelocitylearn- yields the strongest results, with a combination of Min- ing,andsupervisiononsmalltemporalgapscreatesamore SNR and weighting yielding an FID of 9.87 at 1-NFE and favorable foundation for subsequently learning large-gap 10.20 when combining DTD and weighting. This demon- average velocity. Building on these insights, we propose stratesthatthetwocomponentsarecomplementary: veloc- animprovedtrainingstrategythatacceleratesinstantaneous ityaccelerationrapidlyestablishestheinstantaneousveloc- velocity formation and prioritizes small temporal gaps in ityfoundation,whileprogressiveweightingdriveseffective the early training phase before gradually transitioning to averagevelocitylearning. largergaps. Ourapproachachievesfasterconvergence,im- Effect of schedule parameter k. While the pace of our provedone-stepgeneration, andreducedtrainingcost. We progressiveweightings=1\u2212(i/T)kcanbemodulatedby believethesefindingsofferbothadeeperunderstandingof adjusting k, we use k = 1 (linear schedule) in all experi- MeanFlow\u2019s learning behavior and a practical foundation mentsforsimplicity.Toverifytheimpactofthischoice,we fordesigningefficientfew-stepgenerativemodels. 8 References multi-task learning. Advances in Neural Information Pro- cessingSystems(NeurIPS),2021. 5 [1] Krea AI. Krea Realtime 14B: Real-time video generation, [16] Kevin Frans, Danijar Hafner, Sergey Levine, and Pieter 2025. 1 Abbeel. One step diffusion via shortcut models. In Inter- [2] MichaelSAlbergoandEricVanden-Eijnden. Buildingnor- national Conference on Learning Representations (ICLR), malizingflowswithstochasticinterpolants. InInternational 2025. 1,2,3,6 ConferenceonLearningRepresentations(ICLR),2023.1,2, [17] Xin Gao, Li Hu, Siqi Hu, Mingyang Huang, Chaonan Ji, 3 DechaoMeng,JinweiQi,PenchongQiao,ZhenShen,Yafei [3] FanBao,ShenNie,KaiwenXue,YueCao,ChongxuanLi, Song,etal. Wan-S2V:Audio-drivencinematicvideogener- HangSu,andJunZhu.Allareworthwords:AViTbackbone ation. preprintarXiv:2508.18621,2025. 1 fordiffusionmodels.InIEEE/CVFConferenceonComputer [18] Zhengyang Geng, Mingyang Deng, Xingjian Bai, J Zico VisionandPatternRecognition(CVPR),2023. 6 Kolter, and Kaiming He. Mean flows for one-step genera- [4] StephenBatifol, AndreasBlattmann, FredericBoesel, Sak- tivemodeling. AdvancesinNeuralInformationProcessing sham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Systems(NeurIPS),2025. 2,3,6,8,1 Zion English, Patrick Esser, Sumith Kulal, et al. FLUX.1 [19] ZhengyangGeng,AshwiniPokle,WilliamLuo,JustinLin, Kontext:Flowmatchingforin-contextimagegenerationand and J Zico Kolter. Consistency models made easy. Inter- editinginlatentspace. preprintarXiv:2506.15742,2025. 1 national Conference on Learning Representations (ICLR), [5] David Berthelot, Arnaud Autef, Jierui Lin, Dian Ang Yap, 2025. 1,3 Shuangfei Zhai, Siyuan Hu, Daniel Zheng, Walter Talbott, [20] Hyojun Go, Yunsung Lee, Seunghyun Lee, Shinhyeok Oh, andEricGu.TRACT:Denoisingdiffusionmodelswithtran- HyeongdonMoon, andSeungtaekChoi. Addressingnega- sitive closure time-distillation. preprint arXiv:2303.04248, tivetransferindiffusionmodels. AdvancesinNeuralInfor- 2023. 2 mationProcessingSystems(NeurIPS),2023. 2,5,6 [6] Nicholas M Boffi, Michael S Albergo, and Eric Vanden- [21] Hyojun Go, Dominik Narnhofer, Goutam Bhat, Prune Eijnden. Flowmapmatching.", "Siyuan Hu, Daniel Zheng, Walter Talbott, [20] Hyojun Go, Yunsung Lee, Seunghyun Lee, Shinhyeok Oh, andEricGu.TRACT:Denoisingdiffusionmodelswithtran- HyeongdonMoon, andSeungtaekChoi. Addressingnega- sitive closure time-distillation. preprint arXiv:2303.04248, tivetransferindiffusionmodels. AdvancesinNeuralInfor- 2023. 2 mationProcessingSystems(NeurIPS),2023. 2,5,6 [6] Nicholas M Boffi, Michael S Albergo, and Eric Vanden- [21] Hyojun Go, Dominik Narnhofer, Goutam Bhat, Prune Eijnden. Flowmapmatching. preprintarXiv:2406.07507, Truong,FedericoTombari,andKonradSchindler. VIST3A: 2024. 1,3 Text-to-3dbystitchingamulti-viewreconstructionnetwork [7] Nicholas M Boffi, Michael S Albergo, and Eric Vanden- toavideogenerator. preprintarXiv:2510.13454,2025. 1 Eijnden. Howtobuildaconsistencymodel: Learningflow [22] Hyojun Go, Byeongjun Park, Jiho Jang, Jin-Young Kim, maps via self-distillation. Advances in Neural Information Soonwoo Kwon, and Changick Kim. SplatFlow: Multi- ProcessingSystems(NeurIPS),2025. 1,3 viewrectifiedflowmodelfor3dgaussiansplattingsynthesis. [8] AndrewBrock,JeffDonahue,andKarenSimonyan. Large In IEEE/CVF Conference on Computer Vision and Pattern scale GAN training for high fidelity natural image synthe- Recognition(CVPR),2025. 1 sis. In International Conference on Learning Representa- [23] Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Lingjie Liu, and tions(ICLR),2019. 6 JoshuaMSusskind. BOOT:Data-freedistillationofdenois- [9] HuiwenChang,HanZhang,LuJiang,CeLiu,andWilliamT ingdiffusionmodelswithbootstrapping.InICMLWorkshop Freeman. MaskGIT:Maskedgenerativeimagetransformer. onStructuredProbabilisticInference&GenerativeModel- In IEEE/CVF Conference on Computer Vision and Pattern ing,2023. 2 Recognition(CVPR),2022. 6 [24] Jiatao Gu, Tianrong Chen, David Berthelot, Huangjie [10] Jooyoung Choi, Jungbeom Lee, Chaehun Shin, Sungwon Zheng, Yuyang Wang, Ruixiang Zhang, Laurent Dinh, Kim,HyunwooKim,andSungrohYoon. Perceptionpriori- MiguelAngelBautista,JoshSusskind,andShuangfeiZhai. tizedtrainingofdiffusionmodels. InIEEE/CVFConference STARFlow: Scaling latent normalizing flows for high- onComputerVisionandPatternRecognition(CVPR),2022. resolutionimagesynthesis. AdvancesinNeuralInformation 2,5 ProcessingSystems(NeurIPS),2025. 6 [11] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, [25] Tiankai Hang, Shuyang Gu, Chen Li, Jianmin Bao, Dong andLiFei-Fei. Imagenet: Alarge-scalehierarchicalimage Chen,HanHu,XinGeng,andBainingGuo. Efficientdiffu- database.InIEEE/CVFConferenceonComputerVisionand siontrainingviamin-SNRweightingstrategy. InIEEE/CVF PatternRecognition(CVPR),2009. 2,3,6 InternationalConferenceonComputerVision(ICCV),2023. [12] PrafullaDhariwalandAlexanderNichol. Diffusionmodels 1,2,5,6 beatgansonimagesynthesis. AdvancesinNeuralInforma- [26] JonathanHeek,EmielHoogeboom,andTimSalimans.Mul- tionProcessingSystems(NeurIPS),2021. 6 tistepconsistencymodels.preprintarXiv:2403.06807,2024. [13] TimDockhorn, ArashVahdat, andKarstenKreis. GENIE: 1,3 Higher-orderdenoisingdiffusionsolvers. AdvancesinNeu- [27] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, ralInformationProcessingSystems(NeurIPS),2022. 1 Bernhard Nessler, and Sepp Hochreiter. GANs trained by [14] Patrick Esser, Robin Rombach, and Bjorn Ommer. Tam- atwotime-scaleupdateruleconvergetoalocalNashequi- ing transformers for high-resolution image synthesis. In librium.AdvancesinNeuralInformationProcessingSystems IEEE/CVF Conference on Computer Vision and Pattern (NeurIPS),2017. 6 Recognition(CVPR),2021. 6 [28] JonathanHo,AjayJain,andPieterAbbeel. Denoisingdiffu- [15] ChrisFifty,EhsanAmid,ZheZhao,TianheYu,RohanAnil, sionprobabilisticmodels. AdvancesinNeuralInformation andChelseaFinn. Efficientlyidentifyingtaskgroupingsfor ProcessingSystems(NeurIPS),2020. 1,2 9 [29] EmielHoogeboom,JonathanHeek,andTimSalimans.Sim- [42] ChengLuandYangSong. Simplifying,stabilizingandscal- ple diffusion: End-to-end diffusion for high resolution im- ingcontinuous-timeconsistencymodels.InternationalCon- ages. In International Conference on Machine Learning ferenceonLearningRepresentations(ICLR),2025. 1,3 (ICML),2023. 6 [43] ChengLu,YuhaoZhou,FanBao,JianfeiChen,Chongxuan [30] Zheyuan Hu, Chieh-Hsin Lai, Yuki Mitsufuji, and Ste- Li, andJunZhu. DPM-Solver: AfastODEsolverfordif- fano Ermon. CMT: Mid-training for efficient learning of fusionprobabilisticmodelsamplinginaround10steps. Ad- consistency, mean flow, and flow map models. preprint vancesinNeuralInformationProcessingSystems(NeurIPS), arXiv:2509.24526,2025. 3 2022. 1 [31] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, [44] ChengLu,YuhaoZhou,FanBao,JianfeiChen,Chongxuan EliShechtman,SylvainParis,andTaesungPark. Scalingup Li,andJunZhu. PM-Solver++: Fastsolverforguidedsam- GANsfortext-to-imagesynthesis.InIEEE/CVFConference plingofdiffusionprobabilisticmodels.MachineIntelligence onComputerVisionandPatternRecognition(CVPR),2023. Research,22:730\u2013751,2025. 1 6 [45] EricLuhmanandTroyLuhman. Knowledgedistillationin [32] Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. iterative generative models for improved sampling speed. Elucidating the design space of diffusion-based generative preprintarXiv:2101.02388,2021. 1,2 models.AdvancesinNeuralInformationProcessingSystems [46] Simian Luo, Yiqin Tan, Longbo Huang, Jian Li, and (NeurIPS),2022. 1,2 Hang Zhao. Latent consistency models: Synthesizing [33] TeroKarras,MiikaAittala,JaakkoLehtinen,JanneHellsten, high-resolution images with few-step inference. preprint Timo Aila, and Samuli Laine. Analyzing and improving arXiv:2310.04378,2023. 3 the training dynamics of diffusion models. In IEEE/CVF [47] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Conference on Computer Vision and Pattern Recognition ZhenguoLi, andZhihuaZhang. Diff-Instruct: Auniversal (CVPR),2024. 2,5 approachfortransferringknowledgefrompre-traineddiffu- [34] DongjunKim,Chieh-HsinLai,Wei-HsiangLiao,NaokiMu- sion models. Advances in Neural Information Processing rata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Systems(NeurIPS),2023. 1,2 Mitsufuji,andStefanoErmon. Consistencytrajectorymod-", "dynamics of diffusion models. In IEEE/CVF [47] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Conference on Computer Vision and Pattern Recognition ZhenguoLi, andZhihuaZhang. Diff-Instruct: Auniversal (CVPR),2024. 2,5 approachfortransferringknowledgefrompre-traineddiffu- [34] DongjunKim,Chieh-HsinLai,Wei-HsiangLiao,NaokiMu- sion models. Advances in Neural Information Processing rata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Systems(NeurIPS),2023. 1,2 Mitsufuji,andStefanoErmon. Consistencytrajectorymod- [48] NanyeMa,MarkGoldstein,MichaelSAlbergo,NicholasM els: Learning probability flow ode trajectory of diffusion. Boffi, EricVanden-Eijnden, andSainingXie. SiT:Explor- In International Conference on Learning Representations ingflowanddiffusion-basedgenerativemodelswithscalable (ICLR),2024. 1,3 interpolanttransformers. InEuropeanConferenceonCom- [35] Jin-Young Kim, Hyojun Go, Soonwoo Kwon, and Hyun- puterVision(ECCV),2024. 1,6 GyoonKim. Denoisingtaskdifficulty-basedcurriculumfor [49] Chenlin Meng, Robin Rombach, Ruiqi Gao, Diederik training diffusion models. In International Conference on Kingma, Stefano Ermon, Jonathan Ho, and Tim Salimans. LearningRepresentations(ICLR),2025. 2,6,1 On distillation of guided diffusion models. In IEEE/CVF [36] Fei Kong, Jinhao Duan, Lichao Sun, Hao Cheng, Renjing Conference on Computer Vision and Pattern Recognition Xu, Hengtao Shen, Xiaofeng Zhu, Xiaoshuang Shi, and (CVPR),2023. 1,2 KaidiXu. ACT-Diffusion: Efficientadversarialconsistency [50] ByeongjunPark,SangminWoo,HyojunGo,Jin-YoungKim, trainingforone-stepdiffusionmodels.InIEEE/CVFConfer- and Changick Kim. Denoising task routing for diffusion enceonComputerVisionandPatternRecognition(CVPR), models. InInternationalConferenceonLearningRepresen- 2024. 3 tations(ICLR),2024. 6 [37] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and [51] WilliamPeeblesandSainingXie. Scalablediffusionmodels KaimingHe. Autoregressiveimagegenerationwithoutvec- with transformers. In IEEE/CVF International Conference torquantization.AdvancesinNeuralInformationProcessing onComputerVision(ICCV),2023. 3,6 Systems(NeurIPS),2024. 6 [38] Zongrui Li, Minghui Hu, Qian Zheng, and Xudong Jiang. [52] AdityaRamesh,PrafullaDhariwal,AlexNichol,CaseyChu, Connecting consistency distillation to score distillation for andMarkChen. Hierarchicaltext-conditionalimagegener- text-to-3dgeneration.InEuropeanConferenceonComputer ationwithcliplatents. preprintarXiv:2204.06125,2022. 2 Vision(ECCV),2024. 2 [53] Robin Rombach, Andreas Blattmann, Dominik Lorenz, [39] YaronLipman,RickyTQChen,HeliBen-Hamu,Maximil- PatrickEsser,andBjo\u00a8rnOmmer.High-resolutionimagesyn- ianNickel,andMattLe.Flowmatchingforgenerativemod- thesiswithlatentdiffusionmodels.InIEEE/CVFConference eling. InInternationalConferenceonLearningRepresenta- onComputerVisionandPatternRecognition(CVPR),2022. tions(ICLR),2023. 1,2,3 6 [40] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow [54] AmirmojtabaSabour,SanjaFidler,andKarstenKreis.Align straightandfast:Learningtogenerateandtransferdatawith your steps: Optimizing sampling schedules in diffusion rectifiedflow.InInternationalConferenceonLearningRep- models. InInternationalConferenceonMachineLearning resentations(ICLR),2023. 1,2,3 (ICML),2024. 1 [41] YunpengLiu,BoxiaoLiu,YiZhang,XingzhongHou,Guan- [55] Amirmojtaba Sabour, Sanja Fidler, and Karsten Kreis. gluSong,YuLiu,andHaihangYou. Seefurtherwhenclear: Align your flow: Scaling continuous-time flow map distil- Curriculumconsistencymodel.InIEEE/CVFConferenceon lation. AdvancesinNeuralInformationProcessingSystems ComputerVisionandPatternRecognition(CVPR),2025. 2 (NeurIPS),2025. 1,3 10 [56] TimSalimansandJonathanHo. Progressivedistillationfor [72] Fu-Yun Wang, Zhaoyang Huang, Alexander Bergman, fastsamplingofdiffusionmodels. InInternationalConfer- Dazhong Shen, Peng Gao, Michael Lingelbach, Keqiang enceonLearningRepresentations(ICLR),2022. 1,2 Sun, Weikang Bian, Guanglu Song, Yu Liu, et al. Phased [57] TimSalimans,ThomasMensink,JonathanHeek,andEmiel consistency models. Advances in Neural Information Pro- Hoogeboom. Multistepdistillationofdiffusionmodelsvia cessingSystems(NeurIPS),2024. 1,3 momentmatching.AdvancesinNeuralInformationProcess- [73] Kai Wang, Mingjia Shi, Yukun Zhou, Zekai Li, Zhihang ingSystems(NeurIPS),2024. 2 Yuan, Yuzhang Shang, Xiaojiang Peng, Hanwang Zhang, [58] AxelSauer,KatjaSchwarz,andAndreasGeiger.StyleGAN- andYangYou.Acloserlookattimestepsisworthyoftriple XL:Scalingstylegantolargediversedatasets. InACMSig- speed-upfordiffusionmodeltraining.InIEEE/CVFConfer- Graph,2022. 6 enceonComputerVisionandPatternRecognition(CVPR), [59] Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas 2025. 2,5,6 Blattmann, Patrick Esser, and Robin Rombach. Fast high- [74] Zidong Wang, Yiyuan Zhang, Xiaoyu Yue, Xiangyu Yue, resolution image synthesis with latent adversarial diffusion YangguangLi,WanliOuyang,andLeiBai. Transitionmod- distillation. InACMSigGraphAsia,2024. 1,2 els: Rethinking the generative learning objective. preprint arXiv:2509.04394,2025. 1,3 [60] AxelSauer,DominikLorenz,AndreasBlattmann,andRobin Rombach. Adversarial diffusion distillation. In European [75] ChenfeiWu,JiahaoLi,JingrenZhou,JunyangLin,Kaiyuan ConferenceonComputerVision(ECCV),2024. 1,2 Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. preprint [61] JaschaSohl-Dickstein, EricWeiss, NiruMaheswaranathan, arXiv:2508.02324,2025. 1 and Surya Ganguli. Deep unsupervised learning using [76] TianweiYin,Michae\u00a8lGharbi,TaesungPark,RichardZhang, nonequilibrium thermodynamics. In International Confer- Eli Shechtman, Fredo Durand, and Bill Freeman. Im- enceonMachineLearning(ICML),2015. 1,2 proveddistributionmatchingdistillationforfastimagesyn- [62] JiamingSong,ChenlinMeng,andStefanoErmon. Denois- thesis. AdvancesinNeuralInformationProcessingSystems ing diffusion implicit models. In International Conference (NeurIPS),2024. 1,2 onLearningRepresentations(ICLR),2021. 1 [77] Tianwei Yin, Michae\u00a8l Gharbi, Richard Zhang, Eli Shecht- [63] YangSongandPrafullaDhariwal. Improvedtechniquesfor man,FredoDurand,WilliamTFreeman,andTaesungPark. training consistency models. International Conference on One-step diffusion", "International Confer- Eli Shechtman, Fredo Durand, and Bill Freeman. Im- enceonMachineLearning(ICML),2015. 1,2 proveddistributionmatchingdistillationforfastimagesyn- [62] JiamingSong,ChenlinMeng,andStefanoErmon. Denois- thesis. AdvancesinNeuralInformationProcessingSystems ing diffusion implicit models. In International Conference (NeurIPS),2024. 1,2 onLearningRepresentations(ICLR),2021. 1 [77] Tianwei Yin, Michae\u00a8l Gharbi, Richard Zhang, Eli Shecht- [63] YangSongandPrafullaDhariwal. Improvedtechniquesfor man,FredoDurand,WilliamTFreeman,andTaesungPark. training consistency models. International Conference on One-step diffusion with distribution matching distillation. LearningRepresentations(ICLR),2024. 1,3,6 In IEEE/CVF Conference on Computer Vision and Pattern [64] YangSongandStefanoErmon.Generativemodelingbyesti- Recognition(CVPR),2024. 1,2 matinggradientsofthedatadistribution.AdvancesinNeural [78] HanZhangandFanCheng. Improvingconsistencydistilla- InformationProcessingSystems(NeurIPS),2019. 1,2 tionwithrectifiedtrajectories. InInternationalConference [65] YangSong,JaschaSohl-Dickstein,DiederikPKingma,Ab- onArtificialNeuralNetworks(ICANN),2025. 2 hishekKumar,StefanoErmon,andBenPoole. Score-based [79] Huijie Zhang, Aliaksandr Siarohin, Willi Menapace, generative modeling through stochastic differential equa- Michael Vasilkovsky, Sergey Tulyakov, Qing Qu, and Ivan tions. In International Conference on Learning Represen- Skorokhodov. AlphaFlow: Understanding and improving tations(ICLR),2021. 2 MeanFlowmodels. preprintarXiv:2510.20771,2025. 3 [66] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya [80] QinshengZhangandYongxinChen. Fastsamplingofdif- Sutskever.Consistencymodels.InInternationalConference fusionmodelswithexponentialintegrator. InInternational onMachineLearning(ICML),2023. 1,2 ConferenceonLearningRepresentations(ICLR),2023. 1 [67] TrevorStandley,AmirZamir,DawnChen,LeonidasGuibas, [81] Hongkai Zheng, Weili Nie, Arash Vahdat, Kamyar Aziz- JitendraMalik,andSilvioSavarese. Whichtasksshouldbe zadenesheli,andAnimaAnandkumar. Fastsamplingofdif- learned together in multi-task learning? In International fusionmodelsviaoperatorlearning. InInternationalCon- ConferenceonMachineLearning(ICML),2020. 5 ferenceonMachineLearning(ICML),2023. 1,2 [68] JoshuaTianJinTee,KangZhang,HeeSukYoon,Dhanan- [82] TianyiZheng, CongGeng, Peng-TaoJiang, BenWan, Hao jaya Nagaraja Gowda, Chanwoo Kim, and Chang D Yoo. Zhang, Jinwei Chen, Jia Wang, and Bo Li. Non-uniform Physicsinformeddistillationfordiffusionmodels. Transac- timestepsampling: Towardsfasterdiffusionmodeltraining. tionsonMachineLearningResearch(TMLR),2024. 2 InACMInternationalConferenceonMultimedia,2024.2,5 [69] KeyuTian, YiJiang, ZehuanYuan, BingyuePeng, andLi- [83] Tianyi Zheng, Peng-Tao Jiang, Ben Wan, Hao Zhang, Jin- weiWang. Visualautoregressivemodeling: Scalableimage wei Chen, Jia Wang, and Bo Li. Beta-tuned timestep dif- generationvianext-scaleprediction. AdvancesinNeuralIn- fusionmodel. InEuropeanConferenceonComputerVision formationProcessingSystems(NeurIPS),2024. 6 (ECCV),2024. [70] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, [84] TianyiZheng,JiayangZou,Peng-TaoJiang,HaoZhang,Jin- Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianx- wei Chen, Jia Wang, and Bo Li. Bidirectional beta-tuned iaoYang,etal. Wan: Openandadvancedlarge-scalevideo diffusionmodel.IEEETransactionsonPatternAnalysisand generativemodels. preprintarXiv:2503.20314,2025. 1 MachineIntelligence(TPAMI),2025. 2,5,6 [71] Fu-Yun Wang, Zhengyang Geng, and Hongsheng Li. Sta- [85] Linqi Zhou, Stefano Ermon, and Jiaming Song. Inductive ble consistency tuning: Understanding and improving con- momentmatching. InInternationalConferenceonMachine sistencymodels. preprintarXiv:2410.18958,2024. 1,3 Learning(ICML),2025. 1,3,6 11 [86] Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, MingzhangYin,andHaiHuang. Scoreidentitydistillation: Exponentially fast distillation of pretrained diffusion mod- elsforone-stepgeneration. InInternationalConferenceon MachineLearning(ICML),2024. 1,2 [87] MingyuanZhou,ZhendongWang,HuangjieZheng,andHai Huang. Guidedscoreidentitydistillationfordata-freeone- step text-to-image generation. In International Conference onLearningRepresentations(ICLR),2025. 2 12 Understanding, Accelerating, and Improving MeanFlow Training Supplementary Material A.SetupsforObservationalStudy weightingstrategies. Thismechanismrescalesthelossac- cording to the magnitude of the regression error. Specif- Inthissection, weprovideadditionaldetailsfortheobser- ically, let e denote the regression error vector, defined as vationalstudypresentedinSec.4. u (z ,t,t)\u2212v (z |\u03f5)fortheinstantaneousvelocitytermL , \u03b8 t t t v andu (z ,r,t)\u2212sg(u )fortheaveragevelocitytermL . \u03b8 t tgt u Impact of v-learning on u-learning. For the v- MeanFlowcalculatesanadaptiveweightw tonormalize adp pretraining stage, we set the flow matching ratio (FM ra- thelossscale: tio)asFMratio=100%,i.e.,wealwayssampletransitions (cid:18) (cid:19) 1 witht=r. ExceptfortheFMratio(theprobabilityofsam- w =sg , (7) adp (\u2225e\u22252+c)p plingt = r)andthenumberofpretrainingepochs, allhy- 2 perparameters(optimizer,learningrateschedule,batchsize, where c is a small constant for numerical stability and p networkarchitecture, andregularization)arekeptidentical controls the normalization strength. The final per-sample tothoseusedinthemaintraininginSec.6. loss is then computed by multiplying this adaptive weight by the squared L norm of the regression error. Empiri- 2 cally, setting p = 1 in MeanFlow has been demonstrated Impact of u-learning on v-learning. For the training toworkeffectively. Therefore,weadoptthissettinginour setupwhereweexplicitlycontrolthetemporalgap\u2206t,we lossformulation. set the sampling ratio of t = r to zero. Given a randomly sampled(t,r),wefirstcomputetherawgap\u2206t", "this adaptive weight by the squared L norm of the regression error. Empiri- 2 cally, setting p = 1 in MeanFlow has been demonstrated Impact of u-learning on v-learning. For the training toworkeffectively. Therefore,weadoptthissettinginour setupwhereweexplicitlycontrolthetemporalgap\u2206t,we lossformulation. set the sampling ratio of t = r to zero. Given a randomly sampled(t,r),wefirstcomputetherawgap\u2206t =t\u2212r andthenrescale\u2206t rawintoaprescribedtargetrar naw ge. Using Integration of our method. Let La vdp(z t,t) and therescaledgap\u2206t\u02dctogetherwiththeoriginallysampledt, La udp(z t,r,t) denote the per-sample loss terms for the weredefinetherewardtimeasr :=t\u2212\u2206t\u02dc. instantaneous and average velocities, respectively. The adaptivenormalizationwouldbeineffectiveifourproposed weighting schedules\u2014specifically \u03b1(t) for velocity learn- Task affinity analysis. The task affinity score is defined ing and \u03b2(\u2206t,s) for progressive training\u2014 were applied as the cosine similarity between the gradients of the two directlytotherawlosses. Therefore,westrictlyapplythese tasks across training iterations. Concretely, at the end of weightingsaftertheadaptivenormalization. each training epoch, we randomly sample 5K data points Inparticular,whenweintegrateourmethodwithaloss- andcomputethegradientsofthev-lossandtheu-losswith weightingmethod(e.g.,MinSNR[25]),theobjectivefunc- 4 intervals described in Fig 5, respectively. We then com- tionisformulatedas: putethegradientcosinesimilaritywithineachtemporalin- tervalofthelossesandaveragethesevaluesoverallsamples Lweighting =E (cid:2) \u03b2(\u2206t,s)\u00b7Ladp(z ,r,t)\u00b7I(t\u0338=r) ours x,\u03f5,t,r u t (8) andtrainingepochstoobtainthefinaltask-affinityscore. +\u03b1(t)\u00b7Ladp(z ,t)\u00b7I(t=r)(cid:3) . v t B.DetailedIntegrationintoMeanFlow Alternatively, when integrating timestep sampling tech- niques(e.g.,DTD[35]),wherethesamplingdistributionis Recap of MeanFlow. To recall, the MeanFlow [18] ob- modifiedtop (t)insteadofapplyingexplicitlossweight- acc jectiveinEq.4reducestostandardflowmatchingbylearn- ingonv,theobjectivebecomes: ing the instantaneous velocity v if t = r. It can be de- composedintotwocomponents,onefortheaverageveloc- Ls oa um rspling =E x,\u03f5,t\u223cpacc(t),r(cid:2) \u03b2(\u2206t,s)\u00b7La udp(z t,r,t)\u00b7I(t\u0338=r) ity L u(z t,r,t) and the other for the instantaneous velocity +La vdp(z t,t)\u00b7I(t=r)(cid:3) . L v(z t,t): (9) L =E (cid:2) L (z ,r,t)I(t\u0338=r)+L (z ,t)I(t=r)(cid:3) , C.AdditionalExperimentalResults MF x,\u03f5,t,r u t v t (6) C.1.ObservationalStudyonFFHQDataset whereI(\u00b7)istheindicatorfunction,L isappliedonlywhen u tdoesnotcoincidewithr,andL isappliedwhent=r. While our primary analysis was conducted on ImageNet, v Furthermore, MeanFlow introduces additional tech- we verify whether these findings generalize to uncondi- niques for improving its training dynamics. Among these tionalimagegenerationontheFFHQdatasetinthissection. techniques, adaptive loss weighting deserves particular at- Figures8\u201310demonstratethatourkeyobservationsconsis- tention, especially since our method also employs specific tentlyholdonFFHQ. 1 v-0 v-100 v-110 v-120 v-130 60 200 40 k=0.03 100 30 k=0.05 50 k=0.1 k=0.2 k=0:21.35 20 20 10 20 30 40 50 60 70 130 140 150 160 170 180 190 200 Training Epochs Training Epochs Figure 9. Corruption in v-learning disrupts u-learning. 1- 30 NFEFIDwhentrainingwithL whileinjectingGaussiannoise MF scaledbyk\u00b7\u2225v (z |\u03f5)\u2225intothetargetvelocityofL . Evensmall t t v noise (k = 0.03) disrupts v-learning and severely degrades u- 20 learningperformancecomparedtocleantraining(k=0). 160 170 180 190 200 Training Epochs Figure 8. v-learning facilitates u-learning. (Top) 1-NFE FID -0 -150 v v duringu-finetuningaccordingtov-pretrainingepochs. (Bottom) 1-NFEFIDunderafixed200-epochbudgetwithvaryingalloca- 200 tionbetweenv-pretrainingandu-finetuning. Bothsettingsshow 100 thatinvestinginv-learningimprovesu-learningquality. 50 -150: 24.27 v 20 Learning v facilitates learning u. Figure 8 presents re- 10 sults corresponding to those in Fig. 2 of the main paper. 0.1-0.3 0.3-0.5 0.5-0.7 0.7-0.9 t=t r Note that the experimental settings are slightly adjusted to Figure10. Impactof\u2206tofu-learningonv-learning. 32-NFE accommodate the different datasets. Since the number of FIDafter150epochsofufinetuningacrossdifferent\u2206tranges, iterationsperepochdecreasessignificantlyfromImageNet starting from either random initialization (blue) or v-pretrained (5,004steps)toFFHQ(273steps),weincreasethenumber model(orange,150epochs). Small\u2206tenablesconstructingand of training epochs to compensate for the reduced training improvingv,whilelarge\u2206tdegradespretrainedv.Thegreenline volume. denotestheperformanceofthev-pretrainedmodel. In the first setting (Top), we fix the budget for u- finetuning at 70 epochs while varying the duration of the Impact of u-Learning on v-Learning. Figure 10 corre- v-pretrainingacross{0,100,110,120,130}epochs. Inthe spondstoFigure4inthemaintext.", "(blue) or v-pretrained (5,004steps)toFFHQ(273steps),weincreasethenumber model(orange,150epochs). Small\u2206tenablesconstructingand of training epochs to compensate for the reduced training improvingv,whilelarge\u2206tdegradespretrainedv.Thegreenline volume. denotestheperformanceofthev-pretrainedmodel. In the first setting (Top), we fix the budget for u- finetuning at 70 epochs while varying the duration of the Impact of u-Learning on v-Learning. Figure 10 corre- v-pretrainingacross{0,100,110,120,130}epochs. Inthe spondstoFigure4inthemaintext. ToadapttotheFFHQ second setting (Bottom), we fix the total training budget dataset, we extended the training duration for both stages at 200 epochs and allocate {0,100,110,120,130} epochs (pretrainingandfinetuning)from40to150epochs. Specif- to v-pretraining, with the remaining epochs used for u- ically,wecomparemodelsstartingfromeitherrandomini- finetuning. tializationoramodelpretrainedwithv-lossfor150epochs, Consistentwithourmainresults,thefindingsclearlyin- followedby150epochsofu-finetuningrestrictedtospecific dicatethataheavierinvestmentinv-pretrainingyieldsmore \u2206tranges. stableandaccurateu-learning. Inthefixed-budgetscenario ThefindingsalignperfectlywiththoseonImageNet: (Bottom),prioritizingvintheearlystagesismoreeffective \u2022 Small \u2206t supervision forms v: Restricting u-learning andtherebyacceleratesconvergence. Overall,theseresults to small temporal gaps (\u2206t \u2208 [0.1,0.3]) proves to be reinforce the conclusion from the main paper that a well- a viable proxy for v-learning, achieving 32-NFE FID formedinstantaneousvelocityv isanecessaryprerequisite scores comparable to pure v-pretraining (green line) forlearningtheaveragevelocityu. when trained from scratch. Furthermore, it yields addi- tionalperformancegainswhenappliedtothev-pretrained Corruptioninv-learningdisruptsu-learning. Figure9 model, indicating effective refinement of the instanta- corresponds to Fig. 3 in the main paper. Consistent with neousvelocity. theprevioussetup,weextendedthetrainingdurationto200 \u2022 Large \u2206t supervision deteriorates v. In contrast, su- epochs for the FFHQ dataset. We observe trends identi- pervisionwithlargertemporalgaps(\u2206t>0.3)resultsin caltothosereportedinSection4.1: evenwithsmallnoise poor 32-NFE FID across both initializations. This con- (k = 0.03), u-learning severely degrades. This confirms firms that large-\u2206t supervision fails to establish v when that a corrupted instantaneous velocity makes learning the training from scratch and also disrupts an already well- averagevelocitysubstantiallymoredifficult. formedvelocityfield. 2 DIF DIF )derp- ,1=EFN( )derp- ,1=EFN( DIF DIF )derp- ,1=EFN( )derp-v ,23=EFN( Method FID(1-NFE)\u2193 FID(2-NFE)\u2193 MeanFlow-B/2 12.90 9.81 +MinSNR 12.82 9.65 +DTD 12.41 9.61 +L weighting. 12.24 9.52 u +DTD+L weighting. 11.33 9.19 u Table6. Ablationofmethodcomponents. Velocityacceleration methodsandL weightingeachimproveuponvanillaMeanFlow u training,withtheircombinationachievingthebestperformance. C.2.AblationofIndividualComponentsonFFHQ dataset Tofurtherdemonstrateandanalyzetheeffectivenessofour method, we conducted an additional ablation study on the FFHQdataset,mirroringtheexperimentaldesignofTable3 inthemainpaper. Forthisexperiment,weutilizedtheDiT- B/2[51]architectureandtrainedallmodelsfor400epochs. TheresultsaresummarizedinTable6. Consistent with the results on ImageNet, applying ve- locity acceleration methods alone improves upon vanilla MeanFlowtraining(12.90 \u2192 12.41withDTDat1-NFE), demonstrating that rapid formulation of v benefits training performance. However, the improvement from MinSNR is relatively marginal (12.90 \u2192 12.82). As discussed in Section 6.2, this is likely because explicit loss weighting strategiesinterferewiththeadaptivelossweightingmecha- nisminherenttoMeanFlow,consequentlyreducingrobust- nessacrossdifferentmodelscales.Moreover,applyingonly progressive weighting on L improves the performance to u 12.24 at 1-NFE, demonstrating the benefit of proper tem- poral gap scheduling. The combined approach yields the strongest results, achieving an FID of 11.33 at 1-NFE and 9.19at2-NFE. 3"]}